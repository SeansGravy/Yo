name: Yo Local CI Mirror

on:
  push:
    branches: [ main, feature/*, fix/* ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    name: Run pytest and Yo verify
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'

      - name: Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          python3 -m pip install -r requirements.txt
          if [ "$(uname)" = "Darwin" ]; then
            python3 -m pip install --break-system-packages -r requirements.txt
          fi

      - name: Run Yo CLI verify
        run: python3 -m yo.cli verify

      - name: Publish telemetry summary
        if: always()
        run: |
          python3 -m yo.cli explain verify || true
          ANALYTICS=$(python3 -m yo.cli telemetry analyze --json || echo "{}"); export ANALYTICS
          python3 - <<'PY'
import json
import os
from pathlib import Path

logs_dir = Path("data/logs")
summary_path = logs_dir / "test_summary.json"
history_path = logs_dir / "test_history.json"
analytics_env = os.environ.get("ANALYTICS", "{}")
try:
    analytics = json.loads(analytics_env)
except json.JSONDecodeError:
    analytics = {}

if not summary_path.exists():
    print("No test summary produced by verify.")
else:
    summary = json.loads(summary_path.read_text(encoding="utf-8"))
    lines = ["### Yo Validation Summary"]
    lines.append(f"- Status: {summary.get('status', 'unknown')}")
    lines.append(
        f"- Tests: {summary.get('tests_passed', 0)}/{summary.get('tests_total', 0)} passed"
    )
    duration = summary.get("duration_seconds")
    if duration is not None:
        lines.append(f"- Duration: {float(duration):.2f}s")
    pass_rate = summary.get("pass_rate")
    if isinstance(pass_rate, (int, float)):
        lines.append(f"- Pass rate: {pass_rate * 100:.1f}%")
    print("\n".join(lines))
    summary_file = os.environ.get("GITHUB_STEP_SUMMARY")
    if summary_file:
        with open(summary_file, "a", encoding="utf-8") as fh:
            fh.write("\n".join(lines) + "\n")

if history_path.exists():
    history = json.loads(history_path.read_text(encoding="utf-8"))
    print(f"Telemetry history entries: {len(history)}")

if analytics:
    summary_file = os.environ.get("GITHUB_STEP_SUMMARY")
    lines = ["", "### Yo Telemetry Insights"]
    mean_rate = analytics.get("pass_rate_mean")
    if isinstance(mean_rate, (int, float)):
        display_rate = mean_rate * 100 if mean_rate <= 1 else mean_rate
        lines.append(f"- Avg pass rate: {display_rate:.1f}%")
    volatility = analytics.get("pass_rate_volatility")
    if isinstance(volatility, (int, float)):
        lines.append(f"- Pass-rate volatility: {volatility:.3f}")
    duration = analytics.get("duration_average")
    if isinstance(duration, (int, float)):
        lines.append(f"- Avg duration: {duration:.2f}s")
    if summary_file and len(lines) > 2:
        with open(summary_file, "a", encoding="utf-8") as fh:
            fh.write("\n".join(lines) + "\n")
PY

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: yo-test-results
          path: |
            data/logs/test_summary.json
            data/logs/test_history.json
            data/logs/dependency_history.json
            data/logs/telemetry_summary.json
            data/logs/badge.md
            yo_test_results_*.log
