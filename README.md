---

# ğŸ§  Yo RAG Pipeline

A **local Retrieval-Augmented Generation (RAG)** system powered by
**Milvus Lite**, **LangChain**, and **Ollama** â€” everything runs offline.

---

## ğŸš€ Quick Start

```bash
git clone https://github.com/SeansGravy/Yo.git
cd Yo
pip install -U langchain langchain-milvus langchain-ollama pymilvus ollama
ollama pull nomic-embed-text   # embeddings
ollama pull llama3             # or your preferred LLM
```

### Ingest & Query

```bash
python3 -m rag.pipeline --ingest ./docs/
python3 -m rag.pipeline --ask "What is LangChain used for?"
```

âœ… **Output**

```
ğŸ—„ï¸  Using Milvus Lite at ./data/milvus_lite.db
âœ… Connected to Milvus Lite
âœ… Ingestion complete.
ğŸ§  Yo says:
LangChain helps developers connect LLMs to external data and tools.
```

---

## ğŸ“‚ Project Layout

```
Yo/
â”œâ”€â”€ rag/
â”‚   â””â”€â”€ pipeline.py      # main RAG pipeline
â”œâ”€â”€ docs/                # your source files
â”œâ”€â”€ data/                # Milvus Lite .db
â””â”€â”€ USER_GUIDE.md        # full documentation
```

---

## ğŸ§© Tech Stack

| Component         | Role                               |
| ----------------- | ---------------------------------- |
| **Milvus Lite**   | Vector store (embedded, no server) |
| **LangChain**     | Retrieval orchestration            |
| **Ollama**        | Local LLM + embeddings             |
| **Python â‰¥ 3.10** | Runtime                            |

---

## ğŸ› ï¸ Next Steps

* Add more documents under `./docs/`
* Try different Ollama models (`mistral`, `phi3`, etc.)
* Build a small FastAPI/Gradio UI

---

### ğŸ¤ Credits

Built by **Sean & Logos**, 2025
Inspired by LangChain, Milvus, and Ollama open-source communities.

---
